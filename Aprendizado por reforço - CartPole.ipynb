{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao aprendizado por reforço"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports e dowloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T11:51:24.808166Z",
     "start_time": "2020-05-19T11:51:24.806215Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T11:51:26.332969Z",
     "start_time": "2020-05-19T11:51:24.810141Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créditos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementação original:\n",
    "https://ieeexplore-ieee-org.ezp2.lib.umn.edu/stamp/stamp.jsp?tp=&arnumber=6313077\n",
    "\n",
    "Baseado no jupyter-notebook:\n",
    "https://github.com/gsurma/cartpole/blob/master/cartpole.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descrição do problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um poste é preso por uma junta não acionada a um carrinho, que se move ao longo de uma pista sem atrito. O sistema é controlado aplicando uma força de +1 ou -1 ao carrinho. O pêndulo começa na vertical, e o objetivo é impedir que ele caia. Uma recompensa de +1 é fornecida para cada passo no tempo em que o poste permanece na posição vertical. O episódio termina quando o poste está a mais de 15 graus da vertical ou o carrinho se move a mais de 2,4 unidades do centro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T11:51:26.336916Z",
     "start_time": "2020-05-19T11:51:26.334325Z"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T11:51:26.583454Z",
     "start_time": "2020-05-19T11:51:26.339198Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQNSolver:\n",
    "    \n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation='relu'))\n",
    "        self.model.add(Dense(24, activation='relu'))\n",
    "        self.model.add(Dense(self.action_space, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=LEARNING_RATE))\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "            Toma decisões aleatórias em certos momentos,\n",
    "            Prevê o q_value dado o estado atual e\n",
    "            Retorna a ação.\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        \n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "        \n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \"\"\"\n",
    "             Retreina o modelo com novas informações,\n",
    "             Seleciona aleatoriamente momentos na memória.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "             Para cada momento da memória:\n",
    "             Caso o estado não tenha sido terminal, atualiza q_values. \n",
    "             Novo q_value é Recompensa + Com a ação prevista para o próximo estado * Gamma\n",
    "             Se a recompensa for negativa, o q_value para esta ação no proximo estado é diminuido.\n",
    "             Se a recompensa for positiva, o q_value para esta ação no proximo estado é aumentado.\n",
    "             Online learning - O modelo é retreinado a cada momento lembrado.\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            q_update = reward\n",
    "            \n",
    "            if not terminal:\n",
    "                q_update = (reward + GAMMA + np.amax(self.model.predict(state_next)[0]))\n",
    "            \n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            self.model.fit(state, q_values, verbose=0)\n",
    "        \n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T11:51:26.826139Z",
     "start_time": "2020-05-19T11:51:26.593026Z"
    }
   },
   "outputs": [],
   "source": [
    "def cartpole():\n",
    "    \"\"\"\n",
    "        Inicia o ambiente de treino.\n",
    "        Salva tamanho do espaço observado. Neste caso é 4 (Posicao do carro, Velocidade do Carro, Angulo com vertical, \n",
    "                                    Velocidade do Topo).\n",
    "        Salva o tamanho do espaco de ação. Neste caso é 2 (esquerda ou direita).\n",
    "        Inicializa nosso resolvedor:\n",
    "                Reinicia o ambiente a cada interacao.\n",
    "                Transforma o vetor state em matrix.\n",
    "                Desenha o ambiente.\n",
    "                Prevê melhor proxima ação dado estado corrente.\n",
    "                Realiza a ação e atualiza status.\n",
    "                Se o ambiente não foi finalizado a recompensa é positiva, se não negativa.\n",
    "                Transforma o vetor state em matrix.\n",
    "                Faz com que a ação seja lembrada.\n",
    "                Re-treina nosso modelo com a nova experiencia.       \n",
    "    \n",
    "    \"\"\"\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)\n",
    "    episode = 0\n",
    "    \n",
    "    while True:\n",
    "        steps= 0\n",
    "        episode+= 1\n",
    "        \n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        \n",
    "        while True:\n",
    "            env.render()\n",
    "            action = dqn_solver.act(state)\n",
    "            state_next, reward, terminal, info = env.step(action)\n",
    "            reward = reward if not terminal else -reward\n",
    "            \n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            q_values = dqn_solver.model.predict(state)\n",
    "            \n",
    "            print(f'\\tSTEP: {steps} - Valores Q: {q_values}\\n\\tAcao: {action} Recompensa: {reward} Finalizado:{terminal}')\n",
    "            print(f'\\n\\tPROXIMO ESTADO- Pos: {state_next[0][0]} Car Vel:{state_next[0][1]} Angulo:{state_next[0][2]} Vel Top:{state_next[0][3]}')\n",
    "            \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "            dqn_solver.experience_replay()\n",
    "            \n",
    "            state = state_next\n",
    "            steps += 1\n",
    "            if terminal:\n",
    "                break\n",
    "        print(f'Episode: {episode} - Passos no ultimo episodio: {steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T11:52:16.289571Z",
     "start_time": "2020-05-19T11:51:26.832060Z"
    }
   },
   "outputs": [],
   "source": [
    "cartpole()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
